<html><body><div style="text-align: left;">
In this post I want to give an update on the status of the ARM backend for PyPy's JIT and describe some of the issues and details of the backend.</div>
<div class="section" id="current-status">
<br>
<h2>




Current Status</h2>
It has been a more than a year that I have been working on the ARM backend. Now it is in a shape, that we can measure meaningful numbers and also ask for some feedback. Since the <a class="reference external" href="http://morepypy.blogspot.com/2011/01/jit-backend-for-arm-processors.html">last post about the backend</a> we have added support floating point operations as well as for PyPy's framework GC's. Another area of work was to keep up with the constant improvements done in the main development branch, such as out-of-line guards, labels, etc. It has been possible for about a year to cross-translate the PyPy Python interpreter and other interpreters such as <a class="reference external" href="https://bitbucket.org/cfbolz/pyrolog/">Pyrolog</a>, with a JIT, to run benchmarks on ARM. Up until now there remained some hard to track bugs that would cause the interpreter to crash with a segmentation fault in certain cases when running with the JIT on ARM. Lately it was possible to run all benchmarks without problems, but when running the translation toolchain itself it would crash. During the last PyPy sprint in <a class="reference external" href="http://morepypy.blogspot.com/2011/12/leysin-winter-sprint.html">Leysin</a> Armin and I managed to fix several of these hard to track bugs in the ARM backend with the result that, it is now possible to run the PyPy translator on ARM itself (at least unless until it runs out of memory), which is a kind of litmus test for the backend itself and used to crash before. Just to point it out, we are not able to complete a PyPy translation on ARM, because on the hardware we have currently available there is not enough memory. But up to the point we run out of memory the JIT does not hit any issues.<br>
<br></div>
<div class="section" id="implementation-details">
<h2>




Implementation Details</h2>
The hardware requirements to run the JIT on ARM follow those for Ubuntu on ARM which targets ARMv7 with a VFP unit running in little endian mode. The JIT can be translated without floating point support, but there might be a few places that need to be fixed to fully work in this setting. We are targeting the ARM instruction set, because at least at the time we decided to use it seemed to be the best choice in terms of speed while having some size overhead compared to the Thumb2 instruction set. It appears that the Thumb2 instruction set should give comparable speed with better code density but has a few restriction on the number of registers available and the use of conditional execution. Also the implementation is a bit easier using a fixed width instruction set and we can use the full set of registers in the generated code when using the ARM instruction set.<br>
<br></div>
<div class="section" id="the-calling-convention-on-arm">
<h2>




The calling convention on ARM</h2>
The calling convention on ARM uses 4 of the general purpose registers to pass arguments to functions, further arguments are passed on the stack. The presence of a floating point unit is not required for ARM cores, for this reason there are different ways of handling floats with relation to the calling convention. There is a so called soft-float calling convention that is independent of the presence of a floating point unit. For this calling convention floating point arguments to functions are stored in the general purpose registers and on the stack. Passing floats around this way works with software and hardware floating point implementations. But in presence of a floating point unit it produces some overhead, because floating point numbers need to be moved from the floating point unit to the core registers to do a call and moved back to the floating point registers by the callee. The alternative calling convention is the so-called hard-float calling convention which requires the presence of a floating point unit but has the advantage of getting rid of the overhead of moving floating point values around when performing a call. Although it would be better in the long term to support the hard-float calling convention, we need to be able to interoperate with external code compiled for the operating system we are running on. For this reason at the moment we only support the soft-float to interoperate with external code. We implemented and tested the backend on a <a class="reference external" href="http://beagleboard.org/hardware-xM/">BeagleBoard-xM</a> with a <a class="reference external" href="http://www.arm.com/products/processors/cortex-a/cortex-a8.php">Cortex-A8</a> processor running <a class="reference external" href="https://wiki.ubuntu.com/ARM">Ubuntu 11.04 for ARM</a>.<br>
<br></div>
<div class="section" id="translating-for-arm">
<h2>




Translating for ARM</h2>
The toolchain used to translate PyPy currently is based on a <a class="reference external" href="http://maemo.gitorious.org/scratchbox2/pages/Home">Scratchbox2</a>. Scratchbox2 is a cross-compiling environment. Development had stopped for a while, but it seems to have revived again. We run a 32-bit Python interpreter on the host system and perform all calls to the compiler using a Scratchbox2 based environment. A description on how to setup the cross translation toolchain can be found <a class="reference external" href="https://bitbucket.org/pypy/pypy/src/1f07ea8076c9/pypy/doc/arm.rst">here</a>.<br>
<br></div>
<div class="section" id="results">
<h2>




Results</h2>
The current results on ARM, as shown in the graph below, show that the JIT currently gives a speedup of about 3.5 times compared to CPython on ARM. The benchmarks were run on the before mentioned BeagleBoard-xM with a 1GHz ARM Cortex-A8 processor and 512MB of memory. The operating system on the board is Ubuntu 11.04 for ARM. We measured the PyPy interpreter with the JIT enabled and disabled comparing each to CPython Python 2.7.1+ (r271:86832) for ARM. The graph shows the speedup or slowdown of both PyPy versions for the different benchmarks from our benchmark suite normalized to the runtime of CPython. The data used for the graph can be seen below.<br>
<div class="separator" style="clear: both; text-align: center;">
<a href="http://2.bp.blogspot.com/-uckc9tOWgnM/TykHMuuGT9I/AAAAAAAAAKg/J8_fC6RS-QA/s1600/graph.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="258" src="http://2.bp.blogspot.com/-uckc9tOWgnM/TykHMuuGT9I/AAAAAAAAAKg/J8_fC6RS-QA/s400/graph.png" width="400"></a></div>
<br>
The speedup is less than the speedup of 5.2 times we currently  get on x86 on our own benchmark suite (see <a class="reference external" href="http://speed.pypy.org/">http://speed.pypy.org</a> for details). There are several possible reasons for this. Comparing the results for the interpreter without the JIT on ARM and x86 suggests that the interpreter generated by PyPy, without the JIT, has a worse performance when compared to CPython that it does on x86. Also it is quite possible that the code we are generating with the JIT is not yet optimal. Also there are some architectural constraints produce some overhead. One of these differences is the handling of constants, most ARM instructions only support 8 bit (that can be shifted) immediate values, larger constants need to be loaded into a register, something that is not necessary on x86.<br>
<br>
<table border="1" class="docutils"><colgroup></colgroup><colgroup><col width="40%"></colgroup><colgroup><col width="32%"></colgroup><colgroup><col width="28%"></colgroup><tbody valign="top">
<tr><td>Benchmark</td><td>PyPy JIT</td><td>PyPy no JIT</td></tr>
<tr><td>ai</td><td>0.484439780047</td><td>3.72756749625</td></tr>
<tr><td>chaos</td><td>0.0807291691934</td><td>2.2908692212</td></tr>
<tr><td>crypto_pyaes</td><td>0.0711114832245</td><td>3.30112318509</td></tr>
<tr><td>django</td><td>0.0977743245519</td><td>2.56779947601</td></tr>
<tr><td>fannkuch</td><td>0.210423735698</td><td>2.49163632938</td></tr>
<tr><td>float</td><td>0.154275334675</td><td>2.12053281495</td></tr>
<tr><td>go</td><td>0.330483034202</td><td>5.84628320479</td></tr>
<tr><td>html5lib</td><td>0.629264389862</td><td>3.60333138526</td></tr>
<tr><td>meteor-contest</td><td>0.984747426912</td><td>2.93838610037</td></tr>
<tr><td>nbody_modified</td><td>0.236969593082</td><td>1.40027234936</td></tr>
<tr><td>pyflate-fast</td><td>0.367447191807</td><td>2.72472422146</td></tr>
<tr><td>raytrace-simple</td><td>0.0290527461437</td><td>1.97270054339</td></tr>
<tr><td>richards</td><td>0.034575573553</td><td>3.29767342015</td></tr>
<tr><td>slowspitfire</td><td>0.786642551908</td><td>3.7397367403</td></tr>
<tr><td>spambayes</td><td>0.660324379456</td><td>3.29059863111</td></tr>
<tr><td>spectral-norm</td><td>0.063610783731</td><td>4.01788986233</td></tr>
<tr><td>spitfire</td><td>0.43617131165</td><td>2.72050579076</td></tr>
<tr><td>spitfire_cstringio</td><td>0.255538702134</td><td>1.7418593111</td></tr>
<tr><td>telco</td><td>0.102918930413</td><td>3.86388866047</td></tr>
<tr><td>twisted_iteration</td><td>0.122723986805</td><td>4.33632475491</td></tr>
<tr><td>twisted_names</td><td>2.42367797135</td><td>2.99878698076</td></tr>
<tr><td>twisted_pb</td><td>1.30991837431</td><td>4.48877805486</td></tr>
<tr><td>twisted_tcp</td><td>0.927033354055</td><td>2.8161624665</td></tr>
<tr><td>waf</td><td>1.02059811932</td><td>1.03793427321</td></tr>
</tbody></table>
</div>
<br>
<br>
<div class="section" id="the-next-steps-and-call-for-help">
<h2>




The next steps and call for help</h2>
Although there probably still are some remaining issues which have not surfaced yet, the JIT backend for ARM is working. Before we can merge the backend into the main development line there are some things that we would like to do first, in particular it we are looking for a way to run the all PyPy tests to verify that things work on ARM before we can merge. Additionally there are some other longterm ideas. To do this we are looking for people willing to help, either by contributing to implement the open features or that can help us with hardware to test.<br>
<br>
The incomplete list of open topics:<br>
<ul class="simple">
<li>We are looking for a better way to translate PyPy for ARM, than the one describe above. I am not sure if there currently is hardware with enough memory to directly translate PyPy on an ARM based system, this would require between 1.5 or 2 Gig of memory. A fully <a class="reference external" href="http://wiki.qemu.org/Main_Page">QEMU</a> based approach could also work, instead of Scratchbox2 that uses QEMU under the hood.</li>
<li>Test the JIT on different hardware.</li>
<li>Experiment with the JIT settings to find the optimal thresholds for ARM.</li>
<li>Continuous integration: We are looking for a way to run the PyPy test suite to make sure everything works as expected on ARM, here QEMU also might provide an alternative.</li>
<li>A long term plan would be to port the backend to ARMv5 ISA and improve the support for systems without a floating point unit. This would require to implement the ISA and create different code paths and improve the instruction selection depending on the target architecture.</li>
<li>Review of the generated machine code the JIT generates on ARM to see if the instruction selection makes sense for ARM.</li>
<li>Build a version that runs on Android.</li>
<li>Improve the tools, i.e. integrate with <a class="reference external" href="https://bitbucket.org/pypy/jitviewer">jitviewer</a>.</li>
</ul>
So if you are interested or willing to help in any way contact us.</div></body></html>