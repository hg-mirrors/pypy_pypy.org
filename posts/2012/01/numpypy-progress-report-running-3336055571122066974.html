<html><body><p>Hello.</p>
<p>We're excited to let you know about some of the great progress we've made on
NumPyPy: both completeness and performance. In this blog entry we mostly
will talk about performance and how much progress we have made so far.
</p><p><strong>Word of warning:</strong> this
work is in progress -- we're maybe half way to where we want to be and there are
many trivial and not so trivial optimizations to be written. (For example, we
haven't even started to implement important optimizations, like vectorization.)</p>
<div class="section" id="benchmark">
<h1>Benchmark</h1>
<p>We chose a laplace equation solver, based on SciPy's <a class="reference external" href="http://www.scipy.org/PerformancePython">PerformancePython</a> wiki.
Unfortunately, the different implementations on the wiki page accidentally use
two different algorithms, which have different convergences, and very different
performance characteristics on modern computers. As a result, we implemented
our own versions in both C and Python (with and without NumPy). The full source
can be found in <a class="reference external" href="https://bitbucket.org/fijal/hack2/src/default/bench/laplace">fijal's hack</a> repo, all these benchmarks were performed at
revision 18502dbbcdb3.</p>
<p>First, let me describe various algorithms used. Note that some of them contain
PyPy-specific hacks to work around limitations in the current implementation.
These hacks will go away eventually and the performance will improve.
Numerically the algorithms used are identical, however exact data layout in
memory differs between them.</p>
<p><strong>A note about all the benchmarks:</strong> they each were run once, but the
performance is very stable across runs.</p>
<p>Starting with the C version, it implements a trivial laplace transform
using two loops and double-reference memory (array of <tt class="docutils literal">int*</tt>). The double
reference does not matter for performance and the two algorithms are
implemented in <tt class="docutils literal"><span class="pre">inline-laplace.c</span></tt> and <tt class="docutils literal">laplace.c</tt>. They were both compiled
with <tt class="docutils literal">gcc 4.4.5</tt> at <tt class="docutils literal"><span class="pre">-O3</span></tt>. The inline version modifies array in-place while the non-inline version stores results in a copy. That makes them converge at different rate, hence different number of iterations</p>
<p>A straightforward version of those in Python is implemented in <tt class="docutils literal">laplace.py</tt>
using, respectively, <tt class="docutils literal">inline_slow_time_step</tt> and <tt class="docutils literal">slow_time_step</tt>.
<tt class="docutils literal">slow_2_time_step</tt> does the same thing, except it copies arrays in-place
instead of creating new copies. Table below compares running PyPy against C:</p>
<table border="1" class="docutils">
<colgroup>
<col width="35%">
<col width="34%">
<col width="31%">
</colgroup>
<tbody valign="top">
<tr><td>bench</td>
<td>number of iterations</td>
<td>time per iteration</td>
</tr>
<tr><td>laplace C</td>
<td>219</td>
<td>6.3ms</td>
</tr>
<tr><td>inline-laplace C</td>
<td>278</td>
<td>20ms</td>
</tr>
<tr><td>slow python</td>
<td>219</td>
<td>17ms</td>
</tr>
<tr><td>slow 2 python</td>
<td>219</td>
<td>14ms</td>
</tr>
<tr><td>inline_slow python</td>
<td>278</td>
<td>23.7ms</td>
</tr>
</tbody>
</table>
<p>An important thing to notice is the data dependency of the inline
version causes a huge slowdown for the C versions. This is not a severe
disadvantage for us though -- the brain-dead Python version takes longer
and PyPy is not able to take advantage of the knowledge that the data is
independent. The results are in the same ballpark as the C versions --
<strong>15% - 170%</strong> slower, but the algorithm
one chooses matters more than the language. By comparison, the slow versions
take about <strong>5.75s</strong> each on CPython 2.6 per iteration and, by estimation,
are about <strong>200x</strong> slower than the PyPy equivalent, if I had the patience to
measure the full run.</p>
<p>The next step is to use NumPy expressions. The first problem we run into is
that computing the error requires walking the entire array a second time. This
is fairly inefficient in terms of cache access, so I took the liberty of
computing the errors every 15 steps. This results in the convergence being
rounded to the nearest 15 iterations, but speeds things up considerably.
<tt class="docutils literal">numeric_time_step</tt> takes the most braindead approach of replacing the array
with itself, like this:</p>
<pre class="literal-block">
u[1:-1, 1:-1] = ((u[0:-2, 1:-1] + u[2:, 1:-1])*dy2 +
                       (u[1:-1,0:-2] + u[1:-1, 2:])*dx2)*dnr_inv
</pre>
<p>We need 3 arrays here -- one is an intermediate (PyPy only needs one, for all of
those subexpressions), one is a copy for computing the error, and one is the
result. This works automatically because in NumPy <tt class="docutils literal">+</tt> or <tt class="docutils literal">*</tt> creates an
intermediate, while NumPyPy avoids allocating the intermediate if possible.</p>
<p><tt class="docutils literal">numeric_2_time_step</tt> works in pretty much the same way:</p>
<pre class="literal-block">
src = self.u
self.u = src.copy()
self.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +
                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv
</pre>
<p>except the copy is now explicit rather than implicit.</p>
<p><tt class="docutils literal">numeric_3_time_step</tt> does the same thing, but notice one doesn't have to copy
the entire array, it's enough to copy the border pieces and fill rest with
zeros:</p>
<pre class="literal-block">
src = self.u
self.u = numpy.zeros((self.nx, self.ny), 'd')
self.u[0] = src[0]
self.u[-1] = src[-1]
self.u[:, 0] = src[:, 0]
self.u[:, -1] = src[:, -1]
self.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +
                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv
</pre>
<p><tt class="docutils literal">numeric_4_time_step</tt> is the one that tries hardest to resemble the C version.
Instead of doing an array copy, it actually notices that one can alternate
between two arrays. This is exactly what the C version does. The
<tt class="docutils literal">remove_invalidates</tt> call is a PyPy specific hack - we hope to remove this
call in the near future, but, in short, it promises "I don't have any unbuilt
intermediates that depend on the value of the argument", which means one doesn't
have to compute sub-expressions one is not actually using:</p>
<pre class="literal-block">
remove_invalidates(self.old_u)
remove_invalidates(self.u)
self.old_u[:,:] = self.u
src = self.old_u
self.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +
                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv
</pre>
<p>This one is the most comparable to the C version.</p>
<p><tt class="docutils literal">numeric_5_time_step</tt> does the same thing, but notices one doesn't have to copy
the entire array, it's enough to just copy the edges. This is an optimization
that was not done in the C version:</p>
<pre class="literal-block">
remove_invalidates(self.old_u)
remove_invalidates(self.u)
src = self.u
self.old_u, self.u = self.u, self.old_u
self.u[0] = src[0]
self.u[-1] = src[-1]
self.u[:, 0] = src[:, 0]
self.u[:, -1] = src[:, -1]
self.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +
                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv
</pre>
<p>Let's look at the table of runs. As before, <tt class="docutils literal">gcc 4.4.5</tt>, compiled at <tt class="docutils literal"><span class="pre">-O3</span></tt>,
and PyPy nightly 7bb8b38d8563, on an x86-64 machine. All of the numeric methods
run for 226 steps, slightly more than the 219, rounding to the next 15 when the
error is computed.</p>
<table border="1" class="docutils">
<colgroup>
<col width="44%">
<col width="25%">
<col width="31%">
</colgroup>
<tbody valign="top">
<tr><td>benchmark</td>
<td>PyPy</td>
<td>CPython</td>
</tr>
<tr><td>numeric</td>
<td>21ms</td>
<td>35ms</td>
</tr>
<tr><td>numeric 2</td>
<td>14ms</td>
<td>37ms</td>
</tr>
<tr><td>numeric 3</td>
<td>13ms</td>
<td>29ms</td>
</tr>
<tr><td>numeric 4</td>
<td>11ms</td>
<td>31ms</td>
</tr>
<tr><td>numeric 5</td>
<td>9.3ms</td>
<td>21ms</td>
</tr>
</tbody>
</table>
<p>We think that these preliminary results are pretty good. They're not as fast as
the C version (or as fast as we'd like them to be), but we're already much
faster than NumPy on CPython -- almost always by more than 2x on this relatively
real-world example. This is not the end, though. In fact, it's hardly the
beginning! As we continue work, we hope to make even more use of the
high level information that we have. Looking at the assembler generated by
gcc for this example, it's pretty clear we can outperform it thanks to better
aliasing information and hence better possibilities for vectorization.
Stay tuned.</p>
<p>EDIT: fixed the benchmark name
</p>
<p>EDIT2: added info that first table is about PyPy</p>
<p>Cheers,
fijal</p>
</div></body></html>