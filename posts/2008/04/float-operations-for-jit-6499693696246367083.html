<html><body><p>Recently, we taught the JIT x86 backend how to produce code for the x87 floating point coprocessor. This means that JIT is able to nicely speed up float operations <b>(this this is not true for our Python interpreter yet - we did not integrate it yet)</b>. This is the first time we started going beyond what is feasible in <a href="http://psyco.sourceforge.net">psyco</a> - it would take a lot of effort to make floats working on top of psyco, way more than it will take on PyPy.
<br><br>
This work is in very early stage and lives on a <a href="http://codespeak.net/svn/pypy/branch/jit-hotpath">jit-hotpath branch</a>, which includes all our recent experiments on JIT compiler generation, including tracing JIT experiments and huge JIT refactoring.
<br><br>
Because we don't encode the Python's semantics in our JIT (which is really a JIT generator), it is expected that our Python interpreter with a JIT will become fast "suddenly", when our JIT generator is good enough. If this point is reached, we  would also get fast interpreters for Smalltalk or JavaScript with relatively low effort.
<br><br>
Stay tuned.
<br><br>

Cheers,<br>
fijal</p></body></html>