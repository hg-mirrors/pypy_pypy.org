<html><body><p>During the sprint we had various discussions about technical issues as well as planning discussions about how we want to go about things. One of them was about the stability of PyPy, how to ensure stability, how to handle releases and approaches to being more "usable". I will describe this discussion in this post (there are also <a class="reference" href="https://codespeak.net/pypy/extradoc/sprintinfo/gothenburg-2007/qa-notes.txt">minutes</a> of the meeting).



<a href="http://3.bp.blogspot.com/_zICyAWqZbLA/R0lpza_qlMI/AAAAAAAAACc/rHmI6t66aVc/s1600-h/whiteboard_releases.JPG"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5136753182133359810" src="http://3.bp.blogspot.com/_zICyAWqZbLA/R0lpza_qlMI/AAAAAAAAACc/rHmI6t66aVc/s320/whiteboard_releases.JPG" style="margin: 0px auto 10px; display: block; text-align: center; cursor: pointer;"></a><span style="font-style: italic;">The Meetings whiteboard</span>

</p><p><a id="testing" name="testing"></a></p><p><a id="testing" name="testing"><span style="font-size: 130%;"><span style="font-weight: bold;">Testing</span></span>
</a></p>  <p>First we discussed the current situation in terms of testing. PyPy has been extremely testing-oriented from the start, it is being developed almost exclusively in test-driven-development style. To deal with the large number of tests we already have some infrastructure in place:</p> <blockquote> <ul class="simple"> <li>we run all of PyPy's tests <a class="reference" href="http://wyvern.cs.uni-duesseldorf.de/pypytest/summary.html">nightly on a Linux machine</a></li> <li>we translate a PyPy Python interpreter every night and use that to run the <a class="reference" href="http://www2.openend.se/%7Epedronis/pypy-c-test/">CPython compliance tests</a> against it, also on a Linux machine</li> <li>we translate several Python interpreters every night and <a class="reference" href="http://tuatara.cs.uni-duesseldorf.de/benchmark.html">run benchmarks</a> against them on a PowerPC running Mac OS X</li> </ul> </blockquote> <p>As you can see, we are lacking in the Windows testing area, which is an even worse problem because none of the currently active developers has Windows as his primary OS. We should improve this by finding a Windows machine where the tests are run nightly and where we can log in to try bug-fixes quickly. The latter bit is important, we had a nightly windows test run before (thanks to Scott Dial) but it didn't help, because even if you tried to fix a bug you would have to wait until the next night to see whether it worked.</p> <p>Another very serious problem is that of aggregation: we have these various test runs that all have a web interface to check for errors but there is no easy way to find out which tests failed. You have to go to each page and even some sub-pages to see what needs fixing, which is a tedious process. The idea for solving this is aggregate all the available information into some sort of testing-entry-point page that gives a quick overview of the regressions that happened during the night. It's not clear whether we can achieve that with existing tools (buildbots or whatever), but we will investigate that.<a id="releases" name="releases">
</a></p>  <p><span style="font-size: 130%;"><span style="font-weight: bold;">Releases</span></span>
</p><p>The discussion about releases was more on a fundamental and less on a concrete level (especially when it comes to time-frames). We discussed what it means to make a release, because obviously it is more than just taking an SVN revision and putting a tarball of it onto the webpage. During the EU period we were required to make several releases, but those were not really meant to be more than technology previews for the brave adventurers to try. In the future we have the goal to release things that are more stable and hopefully more practically useful. The plan is to use medium-sized Python applications that have a chance to run on top of PyPy because they don't use too many extension modules (web apps being likely candidates) and that have good unit-tests themselves. The first step would be to find some applications that fit this description, fix the bugs that prevents PyPy from running them and from then on run them nightly on one of the testing machines to check for regressions. This would allow us to be more confident when stating that "PyPy works".</p> <p>Another thing to keep in mind for releases is the special features that our Python interpreter provides (e.g. the <a class="reference" href="http://codespeak.net/pypy/dist/pypy/doc/objspace-proxies.html#the-thunk-object-space">thunk</a> and the <a class="reference" href="http://codespeak.net/pypy/dist/pypy/doc/objspace-proxies.html#the-taint-object-space">taint</a> object space, our <a class="reference" href="http://codespeak.net/pypy/dist/pypy/doc/stackless.html">stackless features</a>, <a class="reference" href="http://codespeak.net/pypy/dist/pypy/doc/objspace-proxies.html#tproxy">transparent proxies</a>, <a class="reference" href="http://codespeak.net/pypy/dist/pypy/doc/sandbox.html">sandboxing</a>, <a class="reference" href="http://codespeak.net/pypy/dist/pypy/doc/interpreter-optimizations.html#object-optimizations">special object implementations</a>). Those features are neither tested by the CPython tests nor by any existing applications. Therefore we cannot really be confident that these features work and don't have too many bugs (in fact, the first time somebody really use the become feature of the thunk space in earnest he found <a class="reference" href="http://codespeak.net/pipermail/pypy-dev/2007q4/004260.html">a serious bug</a> that is not fixed so far). To get around this problem, we plan to write small-to-medium sized example applications for each of these features (for stackless we can maybe use one of the existing stackless examples). This will hopefully find bugs and will also make it possible to evaluate whether the features make sense from a language design point of view.</p> <p>A minor thing to make releases easier is to be able to not only have the tests be run once a night but also be able to trigger them manually on the release branch before doing the release.</p><p><a id="publishing-cool-things" name="publishing-cool-things"><span style="font-size: 130%;"><span style="font-weight: bold;">Publishing Cool Things</span></span>
</a></p>  <p>Since we decided that the releases we make should be stable and usable, we also discussed how we would go about making new "cool things" like features, experiments etc. better known. The consensus was that this blog is probably the best forum for doing this. In addition we discussed having a stabler snapshot of the trunk made to ensure that people wanting to play around with these features don't accidentally get
a broken version.</p><p><span style="font-size: 130%;"><span style="font-weight: bold;">Helping Out</span></span>
</p><p>Right now we are still in cleanup mode (the cleanup sprint is nearly done, but we haven't finished all the cleanups yet), so we won't be able to start on the above things <em>right</em> now. However, they will have a strong focus soon. So if you are interested in trying out to run programs on top of PyPy or writing new ones that use the new features you are most welcome to do so and we will try to fix the bugs or help you doing it (of course some tolerance against frustration is needed when you do that, because the bugs that turn up tend to be obscure). We have not been perfect at this in the past, but this will have to change.</p></body></html>