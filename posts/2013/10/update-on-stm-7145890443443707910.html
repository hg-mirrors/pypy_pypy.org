<html><body><p>Hi all,</p>
<p>The sprint in London was a lot of fun and very fruitful. In the last
update on STM, Armin was working on improving and specializing the
automatic barrier placement. There is still a lot to do in that area,
but that work is merged now. Specializing and improving barrier placement
is still to be done for the JIT.</p>
<p>But that is not all. Right after the sprint, we were able to squeeze
the last obvious bugs in the STM-JIT combination. However, the performance
was nowhere near to what we want. So until now, we fixed some of the most
obvious issues. Many come from RPython erring on the side of caution
and e.g. making a transaction inevitable even if that is not strictly
necessary, thereby limiting parallelism. Another problem came from
increasing counters everytime a guard fails, which caused transactions
to conflict on these counter updates. Since these counters do not have
to be completely accurate, we update them non-transactionally now with
a chance of small errors.</p>
<p>There are still many such performance issues of various complexity left
to tackle: we are nowhere near done. So stay tuned or contribute :)</p>

<h2>Performance</h2>
<p>Now, since the JIT is all about performance, we want to at least
show you some numbers that are indicative of things to come.
Our set of STM benchmarks is very small unfortunately
(something you can help us out with), so this is
not representative of real-world performance. We tried to
minimize the effect of JIT warm-up in the benchmark results.</p>
<p>The machine these benchmarks were executed on has 4 physical
cores with Hyper-Threading (8 hardware threads).</p>
<p><strong>Raytracer</strong> from <a class="reference external" href="https://bitbucket.org/Raemi/stm-benchmarks/src">stm-benchmarks</a>:
Render times in seconds for a 1024x1024 image:</p>
<table border="1" class="docutils">
<colgroup>
<col width="23%">
<col width="39%">
<col width="38%">
</colgroup>
<thead valign="bottom">
<tr><th class="head">Interpreter</th>
<th class="head">Base time: 1 thread</th>
<th class="head">8 threads (speedup)</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>PyPy-2.1</td>
<td>2.47</td>
<td>2.56 (0.96x)</td>
</tr>
<tr><td>CPython</td>
<td>81.1</td>
<td>73.4 (1.1x)</td>
</tr>
<tr><td>PyPy-STM</td>
<td>50.2</td>
<td>10.8 (4.6x)</td>
</tr>
</tbody>
</table>
<p>For comparison, disabling the JIT gives 148s on PyPy-2.1 and 87s on
PyPy-STM (with 8 threads).</p>
<p><strong>Richards</strong> from <a class="reference external" href="https://bitbucket.org/pypy/pypy/commits/branch/stmgc-c4">PyPy repository on the stmgc-c4
branch</a>:
Average time per iteration in milliseconds:</p>
<table border="1" class="docutils">
<colgroup>
<col width="23%">
<col width="39%">
<col width="38%">
</colgroup>
<thead valign="bottom">
<tr><th class="head">Interpreter</th>
<th class="head">Base time: 1 thread</th>
<th class="head">8 threads (speedup)</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>PyPy-2.1</td>
<td>15.6</td>
<td>15.4 (1.01x)</td>
</tr>
<tr><td>CPython</td>
<td>239</td>
<td>237 (1.01x)</td>
</tr>
<tr><td>PyPy-STM</td>
<td>371</td>
<td>116 (3.2x)</td>
</tr>
</tbody>
</table>
<p>For comparison, disabling the JIT gives 492ms on PyPy-2.1 and 538ms on
PyPy-STM.</p>

<h2>Try it!</h2>
<p>All this can be found in the <a class="reference external" href="https://bitbucket.org/pypy/pypy/commits/branch/stmgc-c4">PyPy repository on the stmgc-c4
branch</a>.
Try it for yourself, but keep in mind that this is still experimental
with a lot of things yet to come. Only Linux x64 is supported right
now, but contributions are welcome.</p>
<p>You can download a prebuilt binary from here:
<a class="reference external" href="https://bitbucket.org/pypy/pypy/downloads/pypy-oct13-stm.tar.bz2">https://bitbucket.org/pypy/pypy/downloads/pypy-oct13-stm.tar.bz2</a>
(Linux x64 Ubuntu &gt;= 12.04).  This was made at revision bafcb0cdff48.</p>

<h2>Summary</h2>
<p>What the numbers tell us is that PyPy-STM is, as expected,
the only of the three interpreters where multithreading gives a large
improvement in speed.  What they also tell us is that, obviously, the
result is not good enough <em>yet:</em> it still takes longer on a 8-threaded
PyPy-STM than on a regular single-threaded PyPy-2.1.  However, as you
should know by now, we are good at promising speed and delivering it...
years later <tt class="docutils literal"><span class="pre">:-)</span></tt></p>
<p>But it has been two years already since PyPy-STM started, and this is
our first preview of the JIT integration.  Expect major improvements
soon: with STM, the JIT generates code that is completely suboptimal in
many cases (barriers, allocation, and more).  Once we improve this, the
performance of the STM-JITted code should come much closer to PyPy 2.1.</p>
<p>Cheers</p>
<p>Remi &amp; Armin</p></body></html>