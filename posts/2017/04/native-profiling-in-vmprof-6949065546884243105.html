<html><body><p>We are happy to announce a new release for the PyPI package <span>vmprof</span>.<br>
It is now able to capture native stack frames on Linux and Mac OS X to show you bottle necks in compiled code (such as CFFI modules, Cython or C Python extensions). It supports PyPy, CPython versions 2.7, 3.4, 3.5 and 3.6. Special thanks to Jetbrains for funding the native profiling support.<br>
<br>
</p><div class="separator" style="clear: both; text-align: center;">
<a href="https://3.bp.blogspot.com/-94RAR1lkAP8/WNmQn-kpLhI/AAAAAAAAAqE/RXg6T4hptnQtH-8fdi87yh_BI37eN6COQCLcB/s1600/vmprof-logo.png" style="margin-left: 1em; margin-right: 1em;"><img alt="vmprof logo" border="0" src="https://3.bp.blogspot.com/-94RAR1lkAP8/WNmQn-kpLhI/AAAAAAAAAqE/RXg6T4hptnQtH-8fdi87yh_BI37eN6COQCLcB/s1600/vmprof-logo.png" title="vmprof logo"></a></div>
<div class="separator" style="clear: both; text-align: center;">
</div>
<br>
<span style="font-size: large;">What is vmprof?</span><br>
<br>
<span style="font-size: large;"><span style="font-size: small;">If you have already worked with vmprof you can skip the next two section. If not, here is a short introduction:</span></span><br>
<br>
<span style="font-size: large;"><span style="font-size: small;">The goal of vmprof package is to give you more insight into your program. It is a statistical profiler. Another prominent profiler you might already have worked with is cProfile. It is bundled with the Python standard library.</span></span><br>
<br>
<span style="font-size: large;"><span style="font-size: small;">vmprof's distinct feature (from most other profilers) is that it does not significantly slow down your program execution. The employed strategy is statistical, rather than deterministic. Not every function call is intercepted, but it samples stack traces and memory usage at a configured sample rate (usually around 100hz). You can imagine that this creates a lot less contention than doing work before and after each function call.</span></span><br>
<br>
<span style="font-size: large;"><span style="font-size: small;">As mentioned earlier cProfile gives you a complete profile, but it needs to intercept every function call (it is a deterministic profiler). Usually this means that you have to capture and record every function call, but this takes an significant amount time.</span></span><br>
<span style="font-size: large;"><span style="font-size: small;"><br>
</span></span> <span style="font-size: large;"><span style="font-size: small;">The overhead vmprof consumes is roughly 3-4% of your total program runtime or even less if you reduce the sampling frequency. Indeed it lets you sample and inspect much larger programs. If you failed to profile a large application with cProfile, please give vmprof a shot.</span></span><br>
<br>
<span style="font-size: large;"><span style="font-size: small;"><span style="font-size: large;">vmprof.com or PyCharm</span></span></span><br>
<br>
<div>
<div>
There are two major alternatives to the command-line tools shipped with vmprof:</div>
<ul>
<li>A web service on <a href="http://vmprof.com/">vmprof.com</a></li>
<li>PyCharm Professional Edition </li>
</ul>
<div>
While the command line tool is only good for quick inspections, <a href="http://vmprof.com/">vmprof.com</a>
 and PyCharm compliment each other providing deeper insight into your 
program. With PyCharm you can view the per-line profiling results inside
 the editor. With the <a href="http://vmprof.com/">vmprof.com</a> you get a handy visualization of the profiling results as a flame chart and memory usage graph.</div>
</div>
<div>
<br></div>
<div>
<div>
Since the PyPy Team runs and maintains the service on <a href="http://vmprof.com/">vmprof.com</a> (which is by the way free and open-source), I’ll explain some more details here. On <a href="http://vmprof.com/">vmprof.com</a> you can inspect the generated profile interactively instead of looking at console output. What is sent to <a href="http://vmprof.com/">vmprof.com</a>? You can find details <a href="http://vmprof.readthedocs.io/en/latest/data.html" target="_blank">here</a>.</div>
</div>
<br><span style="font-size: large;"><span style="font-size: small;"><b>Flamegraph</b>: </span></span>Accumulates and displays the most frequent codepaths. It allows you to quickly and accurately identify hot spots in your code. The flame graph below is a very short run of richards.py (Thus it shows a lot of time spent in PyPy's JIT compiler).<br>
<br>
<div class="separator" style="clear: both; text-align: center;">
<a href="https://4.bp.blogspot.com/-n5LoH2hf7qI/WNvtNvIAbsI/AAAAAAAAAqc/zn0AXv8fkzIMQXWUwMLtLFpjochspz5MwCLcB/s1600/flamegraph.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="231" src="https://4.bp.blogspot.com/-n5LoH2hf7qI/WNvtNvIAbsI/AAAAAAAAAqc/zn0AXv8fkzIMQXWUwMLtLFpjochspz5MwCLcB/s400/flamegraph.png" width="400"></a></div>
<br>
<br>
<b>List all functions (optionally sorted)</b>: the equivalent of the vmprof command line output in the web.<br>
<br>
<div class="separator" style="clear: both; text-align: center;">
<a href="https://3.bp.blogspot.com/-zzAmBuf-3KM/WNvtNze_sZI/AAAAAAAAAqg/9u4Kxv_OzMsTV7KgRx9PvXGHOAPdfXYUgCLcB/s1600/list-of-functions.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="215" src="https://3.bp.blogspot.com/-zzAmBuf-3KM/WNvtNze_sZI/AAAAAAAAAqg/9u4Kxv_OzMsTV7KgRx9PvXGHOAPdfXYUgCLcB/s400/list-of-functions.png" width="400"></a></div>
<br>
 <b>Memory curve</b>: A line plot that shows how how many MBytes have been consumed over the lifetime of your program (see more info in the section below).<br>
<br>
<div class="separator" style="clear: both; text-align: center;">
<a href="https://cloud.githubusercontent.com/assets/175722/17400119/70d43a84-5a46-11e6-974b-913cfa22a531.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="187" src="https://cloud.githubusercontent.com/assets/175722/17400119/70d43a84-5a46-11e6-974b-913cfa22a531.png" width="400"></a></div>
<span style="font-size: large;"><span style="font-size: small;"><span style="font-size: large;">Native programs</span></span></span><br>
<span style="font-size: large;"><span style="font-size: small;"></span></span><br>
<span style="font-size: large;"><span style="font-size: small;">The new feature introduced in vmprof 0.4.x allows you to look beyond the Python level. As you might know, Python maintains a stack of frames to save the execution. Up to now the vmprof profiles only contained that level of information. But what if you program jumps to native code (such as calling gzip compression on a large file)? Up to now you would not see that information.</span></span><br>
<span style="font-size: large;"><span style="font-size: small;"><br></span></span>
<span style="font-size: large;"><span style="font-size: small;">Many packages make use of the CPython C API (which we discurage, please lookup <a href="https://cffi.readthedocs.org/" target="_blank">cffi</a> for a better way to call C). Have you ever had the issue that you know that your performance problems reach down to, but you could not profile it properly?<b> Now you can!</b></span></span><br>
<span style="font-size: large;"><span style="font-size: small;"><br>
</span></span> <span style="font-size: large;"><span style="font-size: small;">Let's inspect a very simple Python program to find out why a program is significantly slower on Linux than on Mac:</span></span><br>
<br>
<span style="font-size: large;"><span style="font-size: small;"><span>import numpy as np<br>
n = 1000<br>
a = np.random.random((n, n))<br>
b = np.random.random((n, n))<br>
c = np.dot(np.abs(a), b)</span><br>
</span></span><br>
<br>
Take two NxN random matrix objects and create a dot product. The first argument to the dot product provides the absolute value of the random matrix.<br>
<br>
<table border="1" style="border: 1px solid silver;"><tbody>
<tr><td>Run</td><td>Python</td><td>NumPy</td><td>OS</td><td>n=...</td> <td>Took</td> </tr>
<tr> <td>[1]</td><td>CPython 3.5.2</td><td>NumPy 1.12.1</td><td>Mac OS X, 10.12.3</td><td>n=5000</td><td>~9 sec</td></tr>
<tr> <td>[2]</td><td>CPython 3.6.0</td><td>NumPy 1.12.1</td><td>Linux 64, Kernel 4.9.14</td><td>n=1000</td><td>~26 sec</td></tr>
</tbody></table>
<br>
Note that the Linux machine operates on a 5 times smaller matrix, still it takes much longer. What is wrong? Is Linux slow? CPython 3.6.0? Well no, lets inspect and <a href="http://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac" target="_blank">[1]</a> and <a href="http://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c" target="_blank">[2]</a> (shown below in that order).<br>
<div class="separator" style="clear: both; text-align: center;">
<a href="https://3.bp.blogspot.com/-WF-JpMQhJaI/WNvx8CPNpTI/AAAAAAAAAqw/ixZpWng6TDc4kIlEHu9zhqrNX4tx0S4rgCLcB/s1600/macosx-profile-blog.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="105" src="https://3.bp.blogspot.com/-WF-JpMQhJaI/WNvx8CPNpTI/AAAAAAAAAqw/ixZpWng6TDc4kIlEHu9zhqrNX4tx0S4rgCLcB/s400/macosx-profile-blog.png" width="400"></a></div>
<div class="separator" style="clear: both; text-align: center;">
<a href="https://1.bp.blogspot.com/-gjM2uj5Ko_E/WNvx73qcXEI/AAAAAAAAAqs/cMvDfcHQ2eAti4BRU0ldwGQ5M-1_TQ2FACEw/s1600/linux-blog.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="113" src="https://1.bp.blogspot.com/-gjM2uj5Ko_E/WNvx73qcXEI/AAAAAAAAAqs/cMvDfcHQ2eAti4BRU0ldwGQ5M-1_TQ2FACEw/s400/linux-blog.png" width="400"></a></div>
<br>
<a href="http://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c" target="_blank">[2]</a> runs on Linux, spends nearly all of the time in PyArray_MatrixProduct2, if you compare to <a href="http://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac" target="_blank">[1]</a> on Mac OS X, you'll see that a lot of time is spent in generating the random numbers and the rest in cblas_matrixproduct.<br>
<br>
Blas has a very efficient implementation so you can achieve the same on Linux if you install a blas implementation (such as openblas).<br>
<br>
Usually you can spot potential program source locations that take a lot of time and might be the first starting point to resolve performance issues.<br>
<br>
<span style="font-size: large;">Beyond Python programs </span><br>
<br>
It is not unthinkable that the strategy can be reused for native programs. Indeed this can already be done by creating a small cffi wrapper around an entry point of a compiled C program. It would even work for programs compiled from other languages (e.g. C++ or Fortran). The resulting function names are the full symbol name embedded into either the executable symboltable or extracted from the dwarf debugging information. Most of those will be compiler specific and contain some cryptic information.<br>
<br>
<span style="font-size: large;">Memory profiling</span><br>
We thankfully received a code contribution from the company Blue Yonder. They have built a memory profiler (for Linux and Mac OS X) on top of vmprof.com that displays the memory consumption for the runtime of your process.<br>
<br>
You can run it the following way:<br>
<br>
<span>$ python -m vmprof --mem --web script.py</span><br>
<br>
By adding --mem, vmprof will capture memory information and display it in the dedicated view on vmprof.com. You can tha view by by clicking the 'Memory' switch in the flamegraph view.<br>
<br>
<span style="font-size: large;">There is more</span><br>
<br>
Some more minor highlights contained in 0.4.x:<br>
<ul>
<li>VMProf support for Windows 64 bit (No native profiling)</li>
<li>VMProf can read profiles generated by another host system</li>
<li>VMProf is now bundled in several binary wheel for fast and easy installation (Mac OS X, Linux 32/64 for CPython 2.7, 3.4, 3.5, 3.6)</li>
</ul>
<span style="font-size: large;">Future plans - Profile Streaming</span><br>
<br>
vmprof has not reached the end of development. There are many features we could implement. But there is one feature that could be a great asset to many Python developers.<br>
<br>
Continuous delivery of your statistical profile, or in short, profile streaming. One of the great strengths of vmprof is that is consumes very little overhead. It is not a crazy idea to run this in production.<br>
<br>
It would require a smart way to stream the profile in the background to vmprof.com and new visualizations to look at much more data your Python service produces.<br>
<br>
If that sounds like a solid vmprof improvement, don't hesitate to get in touch with us (e.g. IRC #pypy, mailing list pypy-dev, or comment below)<br>
<br>
<span style="font-size: large;">You can help! </span><br>
<br>
There are some immediate things other people could help with. Either by donating time or money (yes we have occasional contributors which is great)!<br>
<ul>
<li>We gladly received code contribution for the memory profiler. But it was not enough time to finish the migration completely. Sadly it is a bit brittle right now.</li>
<li>We would like to spend more time on other visualizations. This should include to give a much better user experience on vmprof.com (like a tutorial that explains the visualization that we already have). </li>
<li>Build Windows 32/64 bit wheels (for all CPython versions we currently support)</li>
</ul>
We are also happy to accept google summer of code projects on vmprof for new visualizations and other improvements. If you qualify and are interested, don't hesitate to ask!<br>
<br>
Richard Plangger (plan_rich) and the PyPy Team<br>
<br>
[1] Mac OS X <a href="http://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac">http://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac</a><br>
[2] Linux64 <a href="http://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c">http://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c</a></body></html>